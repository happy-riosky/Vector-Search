{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a94f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Ollama 在 Python 中的使用指南（中文）\n",
      "\n",
      "Ollama 是一款轻量级的本地 LLM 服务端，支持多种模型（如 Llama‑3、Mistral、Gemma 等）。  \n",
      "它提供了 **Python SDK**（`ollama` 包），可以像调用 OpenAI 接口一样，在本地调用模型。  \n",
      "下面按步骤说明如何在 Python 项目里使用 Ollama。\n",
      "\n",
      "---\n",
      "\n",
      "### 1. 环境准备\n",
      "\n",
      "| 步骤 | 命令 | 说明 |\n",
      "|------|------|------|\n",
      "| 1. 安装 Ollama CLI | `curl -fsSL https://ollama.com/install.sh | sh` 或者 Windows PowerShell `<script>` | 下载并安装本地服务器。 |\n",
      "| 2. 拉取模型 | `ollama pull llama3` | 你可以把 `llama3` 换成你想用的模型名。 |\n",
      "| 3. 启动服务器 | `ollama serve` | 服务器默认监听 `http://localhost:11434`。 |\n",
      "\n",
      "> **提示**：如果你只想在脚本里使用 SDK，可以省略手动启动，SDK 会自动尝试连接本地服务器（需要先在后台运行 `ollama serve`）。\n",
      "\n",
      "---\n",
      "\n",
      "### 2. 安装 Python SDK\n",
      "\n",
      "```bash\n",
      "pip install ollama\n",
      "```\n",
      "\n",
      "> 版本信息：截至 2025‑08‑17，最新 SDK 版本为 `ollama==0.3.0`（实际请查看 PyPI）。\n",
      "\n",
      "---\n",
      "\n",
      "### 3. 基本使用\n",
      "\n",
      "#### 3.1 连接到本地 Ollama 服务器\n",
      "\n",
      "```python\n",
      "import ollama\n",
      "\n",
      "# 直接使用默认本地地址\n",
      "client = ollama.Client(host=\"http://localhost:11434\")\n",
      "```\n",
      "\n",
      "> `Client` 的构造函数支持 `host`、`timeout` 等参数，默认 `host=\"http://localhost:11434\"`。\n",
      "\n",
      "#### 3.2 列出已拉取的模型\n",
      "\n",
      "```python\n",
      "models = client.list()\n",
      "print(models)\n",
      "# [{'name': 'llama3', 'modified_at': '2025-08-17T12:34:56+00:00'}, ...]\n",
      "```\n",
      "\n",
      "#### 3.3 文本生成（Completion）\n",
      "\n",
      "```python\n",
      "prompt = \"请用三句话介绍一下 Ollama。\"\n",
      "\n",
      "resp = client.generate(\n",
      "    model=\"llama3\",\n",
      "    prompt=prompt,\n",
      "    temperature=0.7,      # 0.0-1.0\n",
      "    top_p=0.9,            # 0.0-1.0\n",
      "    stream=False          # 是否流式返回\n",
      ")\n",
      "\n",
      "print(resp[\"response\"])\n",
      "```\n",
      "\n",
      "> **返回格式**：`{'model': 'llama3', 'response': '…', 'created_at': '…'}`\n",
      "\n",
      "#### 3.4 聊天式生成（Chat）\n",
      "\n",
      "```python\n",
      "messages = [\n",
      "    {\"role\": \"system\", \"content\": \"你是一名帮助用户的技术支持助手。\"},\n",
      "    {\"role\": \"user\", \"content\": \"如何在 Python 中使用 Ollama？\"},\n",
      "]\n",
      "\n",
      "resp = client.chat(\n",
      "    model=\"llama3\",\n",
      "    messages=messages,\n",
      "    temperature=0.6,\n",
      "    stream=False\n",
      ")\n",
      "\n",
      "print(resp[\"message\"][\"content\"])\n",
      "```\n",
      "\n",
      "> 与 OpenAI Chat API 的调用方式完全相同。\n",
      "\n",
      "#### 3.5 获取向量（Embedding）\n",
      "\n",
      "```python\n",
      "text = \"Ollama 是一个本地 LLM 服务。\"\n",
      "emb = client.embeddings(\n",
      "    model=\"llama3\",\n",
      "    input=text\n",
      ")\n",
      "print(emb[\"embedding\"])  # 维度为 4096（或根据模型而定）\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 4. 进阶：流式（Streaming）调用\n",
      "\n",
      "流式可以边生成边处理文本，适合聊天 UI。\n",
      "\n",
      "```python\n",
      "for token in client.generate(\n",
      "        model=\"llama3\",\n",
      "        prompt=\"写一首关于春天的诗\",\n",
      "        stream=True):\n",
      "    # token 是字典，{'content': '…'}\n",
      "    print(token[\"content\"], end=\"\", flush=True)\n",
      "print()\n",
      "```\n",
      "\n",
      "> 你可以把它包装成异步生成器，配合 `asyncio` 进一步提升效率。\n",
      "\n",
      "---\n",
      "\n",
      "### 5. 错误处理\n",
      "\n",
      "```python\n",
      "try:\n",
      "    client.generate(...)\n",
      "except ollama.exceptions.APIError as e:\n",
      "    print(f\"Ollama API error: {e}\")\n",
      "except ollama.exceptions.ConnectionError:\n",
      "    print(\"无法连接到 Ollama 服务器，请先执行 `ollama serve`。\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 6. 在 Docker / CI 环境中使用\n",
      "\n",
      "如果你在 Docker 容器或 CI 环境里部署，推荐使用 `ollama serve` 并在后台保持运行：\n",
      "\n",
      "```dockerfile\n",
      "FROM python:3.12-slim\n",
      "\n",
      "RUN pip install ollama\n",
      "COPY ./your_script.py /app/\n",
      "\n",
      "CMD [\"sh\", \"-c\", \"ollama serve & python /app/your_script.py\"]\n",
      "```\n",
      "\n",
      "> `ollama serve &` 让服务器在后台启动，脚本即可立即连接。\n",
      "\n",
      "---\n",
      "\n",
      "### 7. 常见问题\n",
      "\n",
      "| 问题 | 解决方案 |\n",
      "|------|----------|\n",
      "| **Ollama 服务器启动后报 `OSError: [Errno 98] Address already in use`** | 检查端口 11434 是否被占用，或改用 `ollama serve --port 11435`。 |\n",
      "| **模型拉取失败，网络慢** | 使用 `--timeout` 或配置代理：`export http_proxy=http://...`。 |\n",
      "| **Python 代码报 `ModuleNotFoundError: No module named 'ollama'`** | 确认已在同一环境下安装 `pip install ollama`，并激活相应的虚拟环境。 |\n",
      "| **生成文本太长** | 在调用时限制 `max_tokens`（如果 SDK 支持）或自行截断。 |\n",
      "\n",
      "---\n",
      "\n",
      "### 8. 小结\n",
      "\n",
      "1. **安装** Ollama CLI → 拉取模型 → 启动服务器。  \n",
      "2. **安装** `ollama` Python SDK。  \n",
      "3. **使用** `Client` 进行 `generate` / `chat` / `embeddings` 等调用，支持同步/流式。  \n",
      "4. **错误处理** 与 `try/except`，并保持服务器正常运行。  \n",
      "\n",
      "这样，你就可以在本地、完全离线的环境里，像调用 OpenAI API 一样，用 Python 调用 Ollama 进行大模型推理。祝你玩得愉快 🚀！\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# 调用模型生成文本\n",
    "response = ollama.chat(\n",
    "    model='gpt-oss:20b',  # 替换为你已拉取的模型名称\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': '你好！请用中文解释如何在 Python 中使用 Ollama。'},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 输出模型的响应\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62082607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**日语翻译**  \n",
      "あなたは馬鹿ですか？  \n",
      "\n",
      "**罗马音（Romaji）**  \n",
      "Anata wa baka desu ka?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': '用日语翻译并标注罗马音: 你是不是傻? '},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbbee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "examgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
